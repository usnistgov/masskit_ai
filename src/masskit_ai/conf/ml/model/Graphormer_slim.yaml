
#   optimizer adam   adam_betas '(0.9, 0.999)'   adam_eps 1e_8   clip_norm 5.0   weight_decay 0.01 \
#   lr_scheduler polynomial_decay   power 1 
#   lr 2e_4   end_learning_rate 1e_9 \
#   batch_size 64 \
#   fp16 \


Graphormer_slim:
  # settings from cmd line, graphormer_slim_architecture, base_architecture
  attention_dropout: 0.1
  act_dropout: 0.1
  dropout: 0.0
  data_buffer_size: 20
  encoder_layers: 12
  encoder_embed_dim: 80
  encoder_ffn_embed_dim: 80
  encoder_attention_heads: 8
  activation_fn: gelu
  encoder_normalize_before: True
  apply_graphormer_init: True
  share_encoder_input_output_embed: False
  no_token_positional_embeddings: False
  pre_layernorm: False
  # settings from GraphPredictionConfig
  num_classes: 1
  max_nodes: 128
  # 512 * 9
  num_atoms: 4608
  # 512 * 3
  num_edges: 1536
  num_in_degree: 512
  num_out_degree: 512
  num_spatial: 512
  num_edge_dis: 128
  multi_hop_max_dist: 5
  edge_type: multi_hop


# remove head, true in fine_tuning
# remove_head